{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.tsv', sep='\\t', header=0, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train0</td>\n",
       "      <td>I supported Barack Obama. I thought it was abs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train1</td>\n",
       "      <td>what to hell with that!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train2</td>\n",
       "      <td>and the stupidity of the haters continues, thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train3</td>\n",
       "      <td>Alberta has been in debt under the Conservativ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train4</td>\n",
       "      <td>The TV is in Channel Search mode, and I have p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  label\n",
       "0  eng_train0  I supported Barack Obama. I thought it was abs...      0\n",
       "1  eng_train1                            what to hell with that!      1\n",
       "2  eng_train2  and the stupidity of the haters continues, thi...      1\n",
       "3  eng_train3  Alberta has been in debt under the Conservativ...      0\n",
       "4  eng_train4  The TV is in Channel Search mode, and I have p...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "df['tokens'] = df['text'].apply(lambda x: [token.lower() for token in wordpunct_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    62530\n",
      "1    36470\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the word embeddings we will manually create a TF-IDF vectorizer\n",
    "# import math\n",
    "\n",
    "# doc_freq = {}\n",
    "# for tokens in df['tokens']:\n",
    "#     unique_tokens = set(tokens)\n",
    "#     for token in unique_tokens:\n",
    "#         doc_freq[token] = doc_freq.get(token, 0) + 1\n",
    "    \n",
    "# total_docs = len(df)\n",
    "# idf = {}\n",
    "# for token in doc_freq:\n",
    "#     idf[token] = math.log((total_docs + 1) / (doc_freq[token] + 1)) + 1\n",
    "\n",
    "# def compute_tfidf_for_document(tokens):\n",
    "#     if not tokens:\n",
    "#         return []\n",
    "#     tf = {}\n",
    "#     doc_length = len(tokens)\n",
    "#     for token in tokens:\n",
    "#         tf[token] = tf.get(token, 0) + 1\n",
    "#     return [(tf[token]/doc_length) * idf.get(token, 0) for token in tokens]\n",
    "\n",
    "# df['tfidf_embedding'] = df['tokens'].apply(compute_tfidf_for_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Step 1: Build a vocabulary\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "vocab = {token: idx+2 for idx, (token, _) in enumerate(Counter(all_tokens).items())}  # idx+2 to reserve 0 for padding and 1 for unknown\n",
    "vocab['<PAD>'] = 0  # Padding token\n",
    "vocab['<UNK>'] = 1  # Unknown token\n",
    "\n",
    "# Step 2: Convert tokens into indices\n",
    "def tokens_to_indices(tokens):\n",
    "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "df['token_indices'] = df['tokens'].apply(tokens_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train0</td>\n",
       "      <td>I supported Barack Obama. I thought it was abs...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, supported, barack, obama, ., i, thought, i...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 2, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train1</td>\n",
       "      <td>what to hell with that!</td>\n",
       "      <td>1</td>\n",
       "      <td>[what, to, hell, with, that, !]</td>\n",
       "      <td>[84, 38, 85, 86, 13, 43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train2</td>\n",
       "      <td>and the stupidity of the haters continues, thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, the, stupidity, of, the, haters, continu...</td>\n",
       "      <td>[11, 87, 88, 35, 87, 89, 90, 21, 91, 92, 93, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train3</td>\n",
       "      <td>Alberta has been in debt under the Conservativ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[alberta, has, been, in, debt, under, the, con...</td>\n",
       "      <td>[112, 113, 114, 115, 116, 117, 87, 14, 21, 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train4</td>\n",
       "      <td>The TV is in Channel Search mode, and I have p...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, tv, is, in, channel, search, mode, ,, an...</td>\n",
       "      <td>[87, 129, 94, 115, 130, 131, 132, 21, 11, 2, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  label  \\\n",
       "0  eng_train0  I supported Barack Obama. I thought it was abs...      0   \n",
       "1  eng_train1                            what to hell with that!      1   \n",
       "2  eng_train2  and the stupidity of the haters continues, thi...      1   \n",
       "3  eng_train3  Alberta has been in debt under the Conservativ...      0   \n",
       "4  eng_train4  The TV is in Channel Search mode, and I have p...      0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [i, supported, barack, obama, ., i, thought, i...   \n",
       "1                    [what, to, hell, with, that, !]   \n",
       "2  [and, the, stupidity, of, the, haters, continu...   \n",
       "3  [alberta, has, been, in, debt, under, the, con...   \n",
       "4  [the, tv, is, in, channel, search, mode, ,, an...   \n",
       "\n",
       "                                       token_indices  \n",
       "0  [2, 3, 4, 5, 6, 2, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
       "1                           [84, 38, 85, 86, 13, 43]  \n",
       "2  [11, 87, 88, 35, 87, 89, 90, 21, 91, 92, 93, 9...  \n",
       "3  [112, 113, 114, 115, 116, 117, 87, 14, 21, 11,...  \n",
       "4  [87, 129, 94, 115, 130, 131, 132, 21, 11, 2, 1...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 745])\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Update MyDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "        self.token_indices = [torch.tensor(indices, dtype=torch.long) for indices in df[\"token_indices\"].values]\n",
    "        self.max_length = max(len(indices) for indices in self.token_indices) if self.token_indices else 0\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.token_indices[idx]\n",
    "        if len(indices) < self.max_length:\n",
    "            padding = torch.zeros(self.max_length - len(indices), dtype=torch.long)\n",
    "            padded_indices = torch.cat([indices, padding])\n",
    "        else:\n",
    "            padded_indices = indices[:self.max_length]\n",
    "        return padded_indices, self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    indices, labels = zip(*batch)\n",
    "    padded_indices = torch.stack(indices)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_indices, labels\n",
    "\n",
    "class_counts = df['label'].value_counts()\n",
    "total_samples = len(df)\n",
    "class_weights = torch.tensor([total_samples / (len(class_counts) * count) for count in class_counts])\n",
    "\n",
    "# Define the sampler\n",
    "sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(class_weights))\n",
    "\n",
    "dataset = MyDataset(df, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn, sampler=sampler)\n",
    "\n",
    "# Iterate through the DataLoader to check the output\n",
    "for padded_embeddings, labels in dataloader:\n",
    "    print(padded_embeddings.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder_Only_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<PAD>'])  # Embedding layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # Convert indices to embeddings\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.dropout(output)\n",
    "        last_output = output[:, -1, :]  # Get last timestep output\n",
    "        return self.fc(last_output)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6902402639389038\n",
      "Epoch: 2, Loss: 0.6481857895851135\n",
      "Epoch: 3, Loss: 0.5908253192901611\n",
      "Epoch: 4, Loss: 0.5325659513473511\n",
      "Epoch: 5, Loss: 0.48368948698043823\n",
      "Epoch: 6, Loss: 0.41308027505874634\n",
      "Epoch: 7, Loss: 0.777518093585968\n",
      "Epoch: 8, Loss: 0.7674357891082764\n",
      "Epoch: 9, Loss: 0.30194804072380066\n",
      "Epoch: 10, Loss: 0.26162683963775635\n",
      "Epoch: 11, Loss: 0.23390445113182068\n",
      "Epoch: 12, Loss: 1.0104020833969116\n",
      "Epoch: 13, Loss: 0.16679181158542633\n",
      "Epoch: 14, Loss: 2.0059618949890137\n",
      "Epoch: 15, Loss: 0.19270765781402588\n",
      "Epoch: 16, Loss: 0.20669353008270264\n",
      "Epoch: 17, Loss: 0.9078084230422974\n",
      "Epoch: 18, Loss: 0.8973689675331116\n",
      "Epoch: 19, Loss: 1.4234565496444702\n",
      "Epoch: 20, Loss: 0.8388950824737549\n",
      "Epoch: 21, Loss: 0.8127148151397705\n",
      "Epoch: 22, Loss: 0.7647733688354492\n",
      "Epoch: 23, Loss: 0.762736976146698\n",
      "Epoch: 24, Loss: 0.7702496647834778\n",
      "Epoch: 25, Loss: 0.4343477487564087\n",
      "Epoch: 26, Loss: 1.0435521602630615\n",
      "Epoch: 27, Loss: 0.7235577702522278\n",
      "Epoch: 28, Loss: 0.6864772439002991\n",
      "Epoch: 29, Loss: 0.49264368414878845\n",
      "Epoch: 30, Loss: 0.7090701460838318\n",
      "Epoch: 31, Loss: 0.8902243375778198\n",
      "Epoch: 32, Loss: 0.5304341912269592\n",
      "Epoch: 33, Loss: 0.5292371511459351\n",
      "Epoch: 34, Loss: 0.8792678713798523\n",
      "Epoch: 35, Loss: 0.703400731086731\n",
      "Epoch: 36, Loss: 0.7076507806777954\n",
      "Epoch: 37, Loss: 0.7054023742675781\n",
      "Epoch: 38, Loss: 0.5685392618179321\n",
      "Epoch: 39, Loss: 0.5784937143325806\n",
      "Epoch: 40, Loss: 0.7293499708175659\n",
      "Epoch: 41, Loss: 0.7033476233482361\n",
      "Epoch: 42, Loss: 0.7032932043075562\n",
      "Epoch: 43, Loss: 0.566369891166687\n",
      "Epoch: 44, Loss: 0.5755013227462769\n",
      "Epoch: 45, Loss: 0.7185490131378174\n",
      "Epoch: 46, Loss: 0.5518593192100525\n",
      "Epoch: 47, Loss: 0.5602647066116333\n",
      "Epoch: 48, Loss: 0.7081052660942078\n",
      "Epoch: 49, Loss: 0.7029355764389038\n",
      "Epoch: 50, Loss: 0.533673107624054\n",
      "Epoch: 51, Loss: 0.7089263200759888\n",
      "Epoch: 52, Loss: 0.7189923524856567\n",
      "Epoch: 53, Loss: 0.6946907043457031\n",
      "Epoch: 54, Loss: 0.7179208993911743\n",
      "Epoch: 55, Loss: 0.7098124027252197\n",
      "Epoch: 56, Loss: 0.7256472706794739\n",
      "Epoch: 57, Loss: 0.5062259435653687\n",
      "Epoch: 58, Loss: 0.7088646292686462\n",
      "Epoch: 59, Loss: 0.9361273050308228\n",
      "Epoch: 60, Loss: 0.7050127983093262\n",
      "Epoch: 61, Loss: 0.4994218349456787\n",
      "Epoch: 62, Loss: 0.7021133899688721\n",
      "Epoch: 63, Loss: 0.5049757361412048\n",
      "Epoch: 64, Loss: 0.4957870841026306\n",
      "Epoch: 65, Loss: 0.708580732345581\n",
      "Epoch: 66, Loss: 0.49696892499923706\n",
      "Epoch: 67, Loss: 0.7026288509368896\n",
      "Epoch: 68, Loss: 0.7098854184150696\n",
      "Epoch: 69, Loss: 0.7070416212081909\n",
      "Epoch: 70, Loss: 0.7137787938117981\n",
      "Epoch: 71, Loss: 0.7124927043914795\n",
      "Epoch: 72, Loss: 0.4750678539276123\n",
      "Epoch: 73, Loss: 0.7040987014770508\n",
      "Epoch: 74, Loss: 0.7113482356071472\n",
      "Epoch: 75, Loss: 0.46994340419769287\n",
      "Epoch: 76, Loss: 0.46740564703941345\n",
      "Epoch: 77, Loss: 0.47619616985321045\n",
      "Epoch: 78, Loss: 0.45388174057006836\n",
      "Epoch: 79, Loss: 0.7376035451889038\n",
      "Epoch: 80, Loss: 0.757895827293396\n",
      "Epoch: 81, Loss: 0.4359656572341919\n",
      "Epoch: 82, Loss: 0.41329237818717957\n",
      "Epoch: 83, Loss: 0.7348020076751709\n",
      "Epoch: 84, Loss: 0.3795076310634613\n",
      "Epoch: 85, Loss: 0.7612624764442444\n",
      "Epoch: 86, Loss: 1.1551873683929443\n",
      "Epoch: 87, Loss: 0.7757622599601746\n",
      "Epoch: 88, Loss: 0.3926987051963806\n",
      "Epoch: 89, Loss: 0.7282834649085999\n",
      "Epoch: 90, Loss: 0.7805927991867065\n",
      "Epoch: 91, Loss: 0.38899385929107666\n",
      "Epoch: 92, Loss: 0.3834226131439209\n",
      "Epoch: 93, Loss: 0.39652666449546814\n",
      "Epoch: 94, Loss: 1.1823596954345703\n",
      "Epoch: 95, Loss: 0.7788794636726379\n",
      "Epoch: 96, Loss: 0.7539540529251099\n",
      "Epoch: 97, Loss: 0.38448566198349\n",
      "Epoch: 98, Loss: 1.0678176879882812\n",
      "Epoch: 99, Loss: 0.743321418762207\n",
      "Epoch: 100, Loss: 0.4019717872142792\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100  # You can adjust this dimension\n",
    "encoder = Encoder_Only_RNN(vocab_size, embedding_dim, hidden_size=128, num_classes=2)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 100\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0\n",
    "    for embeddings, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden state for each batch\n",
    "        batch_size = embeddings.size(0)\n",
    "        hidden = encoder.init_hidden(batch_size)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = encoder(embeddings, hidden)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {avg_loss / len(dataloader)}\")\n",
    "\n",
    "torch.save(encoder.state_dict(), 'encoder_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3431060606060606\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv('dev.tsv', sep='\\t', header=0, quoting=3)\n",
    "dev_df['tokens'] = dev_df['text'].apply(lambda x: [token.lower() for token in wordpunct_tokenize(x)])\n",
    "dev_df['token_indices'] = dev_df['tokens'].apply(tokens_to_indices)\n",
    "\n",
    "dev_dataset = MyDataset(dev_df, vocab)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "# Testing loop\n",
    "encoder.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in dev_dataloader:\n",
    "        batch_size = embeddings.size(0)\n",
    "        hidden = encoder.init_hidden(batch_size)\n",
    "\n",
    "        logits = encoder(embeddings, hidden)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (metaflow)",
   "language": "python",
   "name": "metaflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
